AUDIO_FILE_NAME="audio_file_name"
# General Settings
MAX_PROCESSES=3 # 1 if using GPU or Docker - Default on CPU = 3
DRY_RUN=False # if True - do not run processing
ASSIGNSPEAKERS=False

FILTER_UNNECESSARY_RTTM=False
MIN_RTTM_DURATION=2.0 # Cut audio less than [seconds]
CLEAR_AUDIO_PARTS_RUN=True # Clear \parts folder each RUN
ENABLE_VOSK_LOGS=False
ENABLE_VOSK_GPU=True

# Custim Vosk model settings
FASTER_DECODING=False #  Fast applying beam 8.0 5000
USE_CUSTOM_VOSK=True
CUSTOM_VOSK_BEAM=8.0 # Default is 13.0, you can try lowering it to 8 or 10 for faster processing, 15.0 prec
CUSTOM_VOSK_MAX_ACTIVE=5000 # Default 7000, precise 10000, faster 5000
CUSTOM_VOSK_LATTICE_BEAM=5.0 # Default 6.0, precise 8.0, faster 5.0
VOSK_SAMPLE_RATE=16000 # Default 16000 Hz
VOSK_MAX_CHUNK_SIZE=20000 # Default 10000 #TODO: fix

USE_BATCHES=True # if False - using splitting by speaker function
BATCH_SIZE=10.0         # Batch size in seconds, can be adjusted

# AI features
ENABLE_AI=True
USE_LOCAL_MODEL=False
# Set this to False to use API calls instead
MODEL_LOAD_TYPE=api
# Use 'api' or 'local' loader type
TOKEN_LIMIT=4096
# token limit (e.g., 4096 for gpt-3.5-turbo)
AI_MODEL_NAME=google-gemini
# model keys: gpt-3.5, gpt-4, llama, deepseek, falcon, mistral
GPT_API_VERSION_1="gpt-3.5-turbo"
GPT_API_VERSION_2="gpt-4"

# [HaggingFace]
USE_SAFETENSORS=True
USE_4BIT=False
USE_CPU_OFFLOAD=True

CUDA_VISIBLE_DEVICES=0

# File Paths (Update with your paths)
WORKSPACE="/path/to/your/workspace"
VOSK_MODEL_PATH="/path/to/vosk/model"
MODEL_NAME="vosk-model-name"
OUTPUT_DIR="./output"
OUTPUT_DIR_PARTS="./audio_files/parts"
AUDIOWORKSPACE="/path/to/audio/workspace"
CONFIG_PATH="./configs"
SCRIPT_PATH="./scripts"

# Model folders handlling
MODEL_OUTER_DIR=/path/to/external/models/storage
MODEL_ENABLE_COPY=True
MODEL_MOVE_AND_DELETE=True
# MODELS_DIR="I:\DEV\models"
MODELS_DIR=./models

# Model Names
VOSK_MODEL_NAME=vosk-model-ru-0.42

# LLAMA_MODEL_NAME=llama-4-8b
LLAMA_MODEL_NAME=llama-2-7b
LLAMA_MODEL_REAL_NAME=nsloth/Meta-Llama-3.1-8B-bnb-4bit

MISTRAL_MODEL_NAME=mistral-7b-v0.3
MISTRAL_MODEL_REAL_NAME=unsloth/Mistral-7B-Instruct-v0.3-bnb-4bit

FALCON_MODEL_NAME=falcon-7b
FALCON_MODEL_REAL_NAME=tiiuae/falcon-7b

DEEPSEEK_MODEL_NAME=DeepSeek-R1-Distill-Llama-8B
DEEPSEEK_MODEL_REAL_NAME=unsloth/DeepSeek-R1-Distill-Llama-8B

STABLE_DIFFUSION_MODEL_NAME=stable-diffusion-v1-5
STABLE_DIFFUSION_MODEL_REAL_NAME=runwayml/stable-diffusion-v1-5

# Model Paths
VOSK_MODEL_PATH=${MODELS_DIR}/vosk/${VOSK_MODEL_NAME}
LLAMA_MODEL_PATH=${MODELS_DIR}/ai/llama/${LLAMA_MODEL_NAME}
MISTRAL_MODEL_PATH=${MODELS_DIR}/ai/mistral/${MISTRAL_MODEL_NAME}
FALCON_MODEL_PATH=${MODELS_DIR}/ai/falcon/${FALCON_MODEL_NAME}
DEEPSEEK_MODEL_PATH=${MODELS_DIR}/ai/deepseek/${DEEPSEEK_MODEL_NAME}
STABLE_DIFFUSION_MODEL_PATH=${MODELS_DIR}/stable_diffusion/${STABLE_DIFFUSION_MODEL_NAME}

# API key
OPENAI_API_KEY="your_API_key_here"
DEEPSEEK_API_KEY="your_API_key_here"
CLOUD_API_KEY="your_API_key_here"
GOOGLE_API_KEY="your_API_key_here"
AUTH_TOKEN_HUGGINFACE="your_API_key"


# WORKSPACE= # Your workspace path
# VOSK_MODEL_PATH= # Path to your Vosk model
# MODEL_NAME= # Vosk model name
# AUDIO_FILE= # Your main audio file (e.g., ZOOM0067.wav)
# OUTPUT_DIR= # Path to your output directory
# OUTPUT_DIR_PARTS= # Path for parts directory
# AUDIO_WORKSPACE= # Your audio workspace path
# DRY_RUN= # Set True or False - if True - do not run processing
# ASSIGNSPEAKERS= # Set True or False
# FILTER_UNNECESSARY_RTTM= # Set True or False
# MIN_RTTM_DURATION= # Min duration for filtering (e.g., 2.0 seconds)
